# -*- coding: utf-8 -*-
"""Copy of mymlproject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PafIWk6eLPrlAt1H821QKwkE4Cn0XDeq
"""

! pip install unidecode

pip install autocorrect

import unidecode
import pandas as pd
import re
import time
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from autocorrect import Speller
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk import word_tokenize
import string
import timeit
import seaborn as sns

stoplist = stopwords.words('english')
stoplist = set(stoplist)
spell = Speller(lang='en')
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

#df=pd.read_csv("/content/zomato_reviews.csv")
df = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)
df #Dataframe that has to be processed

print("No. of rows",df.shape[0])
print("No. of columns",df.shape[1])
print("features",df.columns)

sns.countplot(x='Liked',data=df)

"""**Preprocessing**"""

df.info()
df.isnull().sum()

df.describe()
df=df.dropna() #Drop null values
df.isnull().sum()

type(df['Review'])

review_list=df['Review'].to_list()
review_list

"""**Remove newlines & tabs**"""

nlines=[]
def remove_newlines(text):
  formatted=text.replace('\n',' ').replace('\\n',' ').replace('\s',' ')
  print(formatted)
  nlines.append(formatted)

for i in review_list:
  print("text:  ",i)
  remove_newlines(i)

"""**Strip HTML tags**"""

stripped_tags=[]
def strip_htmltags(text):
  soup=BeautifulSoup(text,'html.parser')
  #print(soup)
  strip_text = soup.get_text(separator=" ")
  print(strip_text)
  stripped_tags.append(strip_text)
for i in nlines:
  print("text:  ",i)
  strip_htmltags(i)

"""**Remove links**"""

rem_links=[]
def remove_links(text):
  https=re.sub(r'http\S+','',text)
  com=re.sub(r'\[A-Za-z]*\.com',' ',https)
  rem_links.append(com)

for i in stripped_tags:
  print("text:  ",i)
  remove_links(i)

"""**Remove white spaces**"""

rwhites=[]
def remove_whitespaces(text):
  whitespace=re.sub(r'\s+',' ',text)
  text=whitespace.replace('?',' ? ').replace(')',') ')
  rwhites.append(text)
for i in rem_links:
  print("text:  ",i)
  remove_whitespaces(i)

"""**Remove Accented characters**"""

acc=[]
def accented_characters_removal(text):

    text = unidecode.unidecode(text)
    acc.append(text)
for i in rwhites:
  print("text:  ",i)
  accented_characters_removal(i)
print(len(acc))

"""**Case conversion**"""

lower=[]
def lower_casing_text(text):

    # Convert text to lower case
    # lower() - It converts all upperase letter of given string to lowercase.
    text = text.lower()
    lower.append(text)
for txt in acc:
  lower_casing_text(txt)
print(lower)

"""**Reduce repeated characters and punctuations**"""

red_repitition=[]
def reducing_repeatation(text):

    # Pattern matching for all case alphabets
    Pattern_alpha = re.compile(r"([A-Za-z])\1{1,}", re.DOTALL)

    # Limiting all the  repeatation to two characters.
    Formatted_text = Pattern_alpha.sub(r"\1\1", text)

    # Pattern matching for all the punctuations that can occur
    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\1{1,}')

    # Limiting punctuations in previously formatted string to only one.
    Combined_Formatted = Pattern_Punct.sub(r'\1', Formatted_text)

    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.
    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)
    red_repitition.append(Final_Formatted)
for txt in lower:
  reducing_repeatation(txt)
print(red_repitition)

"""**Expand contraction words**"""

CONTRACTION_MAP = {
"ain't": "is not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he'll've": "he he will have",
"he's": "he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how is",
"i'd": "i would",
"i'd've": "i would have",
"i'll": "i will",
"i'll've": "i will have",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'd've": "it would have",
"it'll": "it will",
"it'll've": "it will have",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she would",
"she'd've": "she would have",
"she'll": "she will",
"she'll've": "she will have",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"so's": "so as",
"that'd": "that would",
"that'd've": "that would have",
"that's": "that is",
"there'd": "there would",
"there'd've": "there would have",
"there's": "there is",
"they'd": "they would",
"they'd've": "they would have",
"they'll": "they will",
"they'll've": "they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
"we'd": "we would",
"we'd've": "we would have",
"we'll": "we will",
"we'll've": "we will have",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what will",
"what'll've": "what will have",
"what're": "what are",
"what's": "what is",
"what've": "what have",
"when's": "when is",
"when've": "when have",
"where'd": "where did",
"where's": "where is",
"where've": "where have",
"who'll": "who will",
"who'll've": "who will have",
"who's": "who is",
"who've": "who have",
"why's": "why is",
"why've": "why have",
"will've": "will have",
"won't": "will not",
"won't've": "will not have",
"would've": "would have",
"wouldn't": "would not",
"wouldn't've": "would not have",
"y'all": "you all",
"y'all'd": "you all would",
"y'all'd've": "you all would have",
"y'all're": "you all are",
"y'all've": "you all have",
"you'd": "you would",
"you'd've": "you would have",
"you'll": "you will",
"you'll've": "you will have",
"you're": "you are",
"you've": "you have",
}
expanded=[]
# The code for expanding contraction words
def expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):

    # Tokenizing text into tokens.
    list_Of_tokens = text.split(' ')

    for Word in list_Of_tokens:
        # Check whether found word is in dictionary "Contraction Map" or not as a key.
         if Word in CONTRACTION_MAP:
                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.
                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]

    # Converting list of tokens to String.
    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens)
    expanded.append(String_Of_tokens)
for txt in  red_repitition:
   expand_contractions(txt)
print(expanded)

"""**Remove special charectors**"""

rem_spcl_char=[]
def removing_special_characters(text):
    Formatted_Text = re.sub(r"[^a-zA-Z0-9:$-,%.?!]+", ' ', text)
    rem_spcl_char.append(Formatted_Text)
for txt in expanded:
  removing_special_characters(txt)
print(rem_spcl_char)

"""**Remove stopwords**"""

global c
c=0
rem_stpwords=[]
stoplist = stopwords.words('english')
stoplist = set(stoplist)
def removing_stopwords(text):
    global c
    text = repr(text)
    # Text without stopwords
    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist ]
    words_string = ' '.join(No_StopWords)
    if c==0:
       print(words_string)
       c+=1
    rem_stpwords.append(words_string)
for txt in rem_spcl_char:
  removing_stopwords(txt)

print(rem_stpwords[1])

import nltk
nltk.download('punkt')

"""**Lemmatization**"""

w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = WordNetLemmatizer()
lemmat=[]
def lemmatization(text):

    # Converting words to their root forms
    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]
    l=' '.join(lemma)
    lemmat.append(l)
for i in rem_stpwords:
  lemmatization(i)
print(lemmat)

"""**Word cloud**"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd
stopwords = set(STOPWORDS)
comment_words=""
comment_words += " ".join(lemmat)+" "

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

import nltk
nltk.download('wordnet')

nltk.download('omw-1.4')

df['Processed_Title'] = lemmat

df['Processed_Title']

df.head()

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
#tokenizer to remove unwanted elements from out data like symbols and numbers
token = RegexpTokenizer(r'[a-zA-Z0-9]+')
cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)
text_counts= cv.fit_transform(df['Processed_Title'])

"""**Train Test Split**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    text_counts, df['Liked'], test_size=0.25, random_state=1)
y_train

"""**Random Forest**"""

from sklearn.ensemble import RandomForestClassifier
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Generation Using Random Forest
mdl=RandomForestClassifier()
clf =mdl.fit(X_train, y_train)
predicted= clf.predict(X_test)
print("Random Forest Accuracy:",metrics.accuracy_score(y_test, predicted))

from sklearn.feature_extraction.text import TfidfVectorizer
tf=TfidfVectorizer()
text_tf= tf.fit_transform(df['Processed_Title'])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    text_tf,df['Liked'], test_size=0.3, random_state=123)

"""**Naive Bayes**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
# Model Generation Using Multinomial Naive Bayes
clf = mdl.fit(X_train, y_train)
predicted= clf.predict(X_test)
print("MultinomialNB Accuracy:",metrics.accuracy_score(y_test, predicted))

"""**SVM**"""

from sklearn.svm import SVC
from sklearn import metrics
m=SVC()
# Model Generation Using SVM
clf = m.fit(X_train, y_train)
predicted= clf.predict(X_test)
print("SVM Accuracy:",metrics.accuracy_score(y_test, predicted))

"""**Metrics**"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import numpy as np
matrix=confusion_matrix(y_test,predicted)
sns.heatmap(matrix/np.sum(matrix),annot=True,fmt=".2%",cmap="Blues")
plt.ylabel("Actuals")
plt.xlabel("Predicted")
plt.title("Confusion matrix for SVM")
plt.show()
matrix

from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve
fpr,tpr,threshold=roc_curve(y_test,predicted)
plt.plot(fpr,tpr)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("ROC curve")
plt.show()

